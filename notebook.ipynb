{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News-Project\n",
    "#### Paul Wecker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Getting data\n",
    "- collected data from freitag.de, a german, left-oriented newspaper\n",
    "- for gathering data, I initially used [`newspaper3k`](https://newspaper.readthedocs.io/en/latest/)\n",
    "    - Problem: article-texts were not fetched properly\n",
    "    - [`newspaper4k`](https://www.reddit.com/r/Python/comments/1bmtdy0/i_forked_newspaper3k_fixed_bugs_and_improved_its/?tl=de) also did not work\n",
    "    - Workaround: use `article_url` fetched from `newspaper4k`, then use `BeautifulSoup` to access `div` with text\n",
    "- found access to paywall content!\n",
    "- collected data on 9 days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](paywall_content.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset\n",
    "Article Data collected comprises:\n",
    "- URL (`newspaper4k`)\n",
    "- Title(`newspaper4k`)\n",
    "- Author(s) (`newspaper4k`)\n",
    "- Text (`BeautifulSoup` free-part and paywall part put together) \n",
    "- Date (`BeautifulSoup` or inferred)\n",
    "- Paywall (boolean) (`BeautifulSoup`)\n",
    "\n",
    "- 311 articles\n",
    "- 185 authors (combination of authors)\n",
    "- character-mean of texts: 6.8k\n",
    "- 171 paywall articles vs 140 free articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Helper functions for getting data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from newspaper import Article\n",
    "import datetime\n",
    "\n",
    "def get_current_date():\n",
    "    today = datetime.date.today()\n",
    "    # Format the current date as DD MM YYYY\n",
    "    return today.strftime('%d_%m_%Y')\n",
    "\n",
    "def convert_ausgabe_string_to_date(week_year_str:str):\n",
    "    # Remove the 'Ausgabe ' prefix and split the input string into week and year\n",
    "    prefix = 'Ausgabe '\n",
    "    if week_year_str.startswith(prefix):\n",
    "        week_year_str = week_year_str[len(prefix):]\n",
    "\n",
    "    week_str, year_str = week_year_str.split('/')\n",
    "    week = int(week_str)\n",
    "    year = int(year_str)\n",
    "    \n",
    "    # Calculate the Monday of the week (using isocalendar)\n",
    "    first_day_of_year = datetime.date(year, 1, 1)\n",
    "    # If the first day of the year is not Monday, adjust to the first Monday\n",
    "    first_monday_of_week = first_day_of_year + datetime.timedelta(days=(week - 1) * 7)\n",
    "    while first_monday_of_week.isocalendar()[2] != 1:  # 1 is Monday\n",
    "        first_monday_of_week += datetime.timedelta(days=1)\n",
    "    \n",
    "    # Calculate the Thursday of that week\n",
    "    thursday_of_week = first_monday_of_week + datetime.timedelta(days=3)\n",
    "    \n",
    "    # Return the date formatted as DD MM YYYY\n",
    "    return thursday_of_week.strftime('%d %m %Y')\n",
    "\n",
    "def retrieve_date_paywall_text(article_html):\n",
    "    soup = BeautifulSoup(article_html, 'html.parser')\n",
    "\n",
    "    # TEXT AND PAYWALL\n",
    "    # only for non-paywall articles\n",
    "    text_class = \"column s-article-text js-dynamic-advertorial js-external-links\"\n",
    "    text = (soup\n",
    "            .find(\"div\",\n",
    "                  {\"class\": text_class}))\n",
    "    # get paywall-article introduction paragraph\n",
    "    intro_class = \"column s-article-text c-paywall-hidden-text js-dynamic-advertorial js-external-links\"\n",
    "    paywall_intro = (soup\n",
    "                     .find(\"div\",\n",
    "                           {\"class\": intro_class}))\n",
    "    \n",
    "    # get content behind paywall\n",
    "    paywall_class= \"o-paywall\"\n",
    "    paywall_text = (soup\n",
    "                    .find('div',\n",
    "                          {'class': paywall_class}))\n",
    "    paywall = False\n",
    "    \n",
    "    # check what has been collected\n",
    "    if text:\n",
    "        text = text.get_text()\n",
    "    elif paywall_intro and paywall_text:\n",
    "        paywall = True\n",
    "        paywall_intro = paywall_intro.get_text()\n",
    "        paywall_text = paywall_text.get_text()[12:] # first chars are \"\\n         \"\n",
    "\n",
    "        def combine_strings(str1, str2):\n",
    "            # Find the longest suffix of str1 that matches the prefix of str2\n",
    "            overlap_len = 0\n",
    "            for i in range(1, len(str1) + 1):\n",
    "                if str2.startswith(str1[-i:]):\n",
    "                    overlap_len = i\n",
    "            \n",
    "            # Combine the strings by removing the overlapping part from str2\n",
    "            combined_string = str1 + str2[overlap_len:]\n",
    "            return combined_string\n",
    "        text = combine_strings(paywall_intro, paywall_text)\n",
    "\n",
    "    # Fetch Date\n",
    "    date = soup.find(\"span\", class_=\"js-article-issue-name\")\n",
    "    if date:\n",
    "        date = date.get_text()\n",
    "        date = convert_ausgabe_string_to_date(date)\n",
    "    else:\n",
    "        date = get_current_date()\n",
    "\n",
    "    return date, paywall, text\n",
    "\n",
    "def fetch_article(article_url):\n",
    "    article = Article(article_url)\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    authors = article.authors\n",
    "    title: str = article.title\n",
    "    date, paywall, text = retrieve_date_paywall_text(article.html)\n",
    "    return [article_url, title, authors, date, paywall, text]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Scripts for scraping and putting things together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pwecker/dev/news/venv4k/lib/python3.12/site-packages/newspaper/source.py:260: FutureWarning: The behavior of this method will change in future versions. Use specific 'len(elem)' or 'elem is not None' test instead.\n",
      "  if feed.doc:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Collecting Article #1 ----\n",
      "---- Collecting Article #2 ----\n",
      "---- Collecting Article #3 ----\n"
     ]
    }
   ],
   "source": [
    "from newspaper import build\n",
    "import pandas as pd\n",
    "\n",
    "from freitag import fetch_article\n",
    "from freitag import get_current_date\n",
    "\n",
    "\n",
    "freitag = build(\"https://www.freitag.de\", language=\"de\", memorize_articles=False)\n",
    "article_urls = freitag.article_urls()\n",
    "\n",
    "df = (pd.DataFrame(data={\"url\":[], \"title\":[], \"authors\":[], \"date\":[],\n",
    "                        \"paywall\":[], \"text\":[]})\n",
    "    .set_index(\"url\"))\n",
    "\n",
    "# for i, article_url in enumerate(article_urls):\n",
    "for i, article_url in enumerate(article_urls[:3]):\n",
    "    print(f\"---- Collecting Article #{i+1} ----\")\n",
    "    row = pd.Series(fetch_article(article_url),\n",
    "                    index=[\"url\", \"title\", \"authors\", \"date\", \"paywall\", \"text\"])\n",
    "    if article_url not in df.index:\n",
    "        df.loc[article_url] = row\n",
    "    else:\n",
    "        print(f\"---- Article {i+1} already stored ----\")\n",
    "\n",
    "# df.to_csv(f\"./data/freitag_{get_current_date()}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "353\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "file_list = sorted(os.listdir(\"data\"))\n",
    "\n",
    "all_dfs= [pd.read_csv(\"data/\"+file) for file in file_list]\n",
    "for df in all_dfs:\n",
    "    df.index = df[\"url\"]\n",
    "\n",
    "df = pd.concat(all_dfs, axis=0)\n",
    "df = df[~df.index.duplicated(keep='first')]\n",
    "df = df[[\"url\", \"title\", \"authors\", \"date\", \"paywall\", \"text\"]]\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Preprocessing\n",
    "- `text`/`title` column\n",
    "- removed some authors\n",
    "- removal of special characters -> new columns\n",
    "- tokenization, stop word removal and lemmatization -> new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3h/sbptm41s5b10cqlvkhtzl1_h0000gn/T/ipykernel_26739/2303015821.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['cleaned_title'] = df['title'].apply(remove_special_characters)\n",
      "/var/folders/3h/sbptm41s5b10cqlvkhtzl1_h0000gn/T/ipykernel_26739/2303015821.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['cleaned_text'] = df['text'].apply(remove_special_characters)\n",
      "/var/folders/3h/sbptm41s5b10cqlvkhtzl1_h0000gn/T/ipykernel_26739/2303015821.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['processed_title'] = df['cleaned_title'].apply(spacy_process_text)\n",
      "/var/folders/3h/sbptm41s5b10cqlvkhtzl1_h0000gn/T/ipykernel_26739/2303015821.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['processed_text'] = df['cleaned_text'].apply(spacy_process_text)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import re\n",
    "\n",
    "df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "df['text'] = df['text'].fillna('')\n",
    "df[\"text\"] = df[\"text\"].str.lower()\n",
    "df[\"title\"] = df[\"title\"].str.lower()\n",
    "df['text_length'] = df['text'].apply(len)\n",
    "authors_to_remove = [\"['Freitag-Veranstaltungen']\", \"[]\", \"['der Freitag Podcast']\"]\n",
    "df = df[~df[\"authors\"].isin(authors_to_remove)]\n",
    "\n",
    "# Function to remove special characters\n",
    "def remove_special_characters(text):\n",
    "    return re.sub(r'[^a-zA-Z0-9\\säöüßÄÖÜ]', '', text)\n",
    "\n",
    "df['cleaned_title'] = df['title'].apply(remove_special_characters)\n",
    "df['cleaned_text'] = df['text'].apply(remove_special_characters)\n",
    "\n",
    "\n",
    "# Load the German spaCy model\n",
    "nlp = spacy.load('de_core_news_sm')\n",
    "\n",
    "# Function to process text using spaCy for tokenization, stop word removal, and lemmatization\n",
    "def spacy_process_text(text):\n",
    "    doc = nlp(text)\n",
    "    lemmatized_tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "df['processed_title'] = df['cleaned_title'].apply(spacy_process_text)\n",
    "df['processed_text'] = df['cleaned_text'].apply(spacy_process_text)\n",
    "\n",
    "df.to_csv(\"data/combined_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. TF-IDF/K-Means\n",
    "- create TF-IDF Features for later clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(311, 39783)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize the TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the 'processed_text' column to create TF-IDF features\n",
    "tfidf_features = tfidf_vectorizer.fit_transform(df['processed_text'])\n",
    "tfidf_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- use K-Means (K=6) to cluster algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 3 3 3 3 3 2 0 4 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3h/sbptm41s5b10cqlvkhtzl1_h0000gn/T/ipykernel_26739/511158530.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"cluster\"] = cluster_labels\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "k = 6\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "kmeans.fit(tfidf_features)\n",
    "\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "# Print the first few labels to get an idea of the cluster assignment\n",
    "print(cluster_labels[:10])\n",
    "df[\"cluster\"] = cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0: ['strom', 'prozent', 'inflation', 'zins', 'mitarbeitende', 'fed', 'führen', 'trump', 'chakma', 'milei']\n",
      "Cluster 1: ['quiz', 'loadingweiterrätselnwenn', 'beweis', 'fallen', 'thema', 'wissen', 'stellen', 'promotour', 'prominente', 'prominenter']\n",
      "Cluster 2: ['china', 'chinesisch', 'vw', 'unternehmen', 'subvention', 'deutsch', 'deutschland', 'milliarde', 'wolfsburg', 'prozent']\n",
      "Cluster 3: ['afd', 'partei', 'prozent', 'mensch', 'bsw', 'politisch', 'linker', 'deutschland', 'thüringen', 'land']\n",
      "Cluster 4: ['roman', 'film', 'geisel', 'frau', 'leben', 'hamas', 'bild', 'mensch', 'israelisch', 'welt']\n",
      "Cluster 5: ['mensch', 'sport', 'emily', 'leben', 'loading', 'sprechen', 'paris', 'sprache', 'einfach', 'deutschland']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Extract feature names\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Calculate mean TF-IDF per cluster\n",
    "df_tfidf = pd.DataFrame(tfidf_features.toarray(), columns=feature_names)\n",
    "df_tfidf['cluster'] = cluster_labels\n",
    "\n",
    "top_words_per_cluster = {}\n",
    "\n",
    "for cluster in range(k):\n",
    "    cluster_data = df_tfidf[df_tfidf['cluster'] == cluster]\n",
    "    \n",
    "    mean_scores = cluster_data.drop('cluster', axis=1).mean(axis=0)\n",
    "    \n",
    "    # get top 10 words for cluster i\n",
    "    top_words = mean_scores.sort_values(ascending=False).head(10).index.tolist()\n",
    "    top_words_per_cluster[cluster] = top_words\n",
    "\n",
    "for cluster, words in top_words_per_cluster.items():\n",
    "    print(f'Cluster {cluster}: {words}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cluster\n",
       "3    152\n",
       "5     61\n",
       "4     46\n",
       "2     25\n",
       "0     18\n",
       "1      9\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"cluster\"].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
